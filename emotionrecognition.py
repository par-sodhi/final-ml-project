import cv2
from keras.models import model_from_json
import numpy as np

# Function to get emotion label from numerical label
def get_emotion_label(label):
    emotions = {
        0: 'angry',
        1: 'disgust',
        2: 'fear',
        3: 'happy',
        4: 'neutral',
        5: 'sad',
        6: 'surprise'
    }
    return emotions.get(label, 'unknown')

def predict_emotion(model, features):
    predicted_values = model.predict(features)
    predicted_emotion = get_emotion_label(predicted_values.argmax())
    return predicted_emotion

def display_predicted_emotion(im, predicted_emotion, x, y):
    cv2.putText(im, '%s' % predicted_emotion, (x - 10, y - 10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255))

#This is loading the trained model 
try:
    json = open("emotion_model.json", "r")
    model_json = json.read()
    json.close()
    model = model_from_json(model_json)
    model.load_weights("emotion_model.weights")
except (IOError, ValueError) as e:
    print(f"Error loading model: {e}")
    exit()

# Loading Haar Cascade for face detection
file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
face_cascade = cv2.CascadeClassifier(file)

#webcam connection
webcam = cv2.VideoCapture(0)

#stores emotion for Spotify 
last_emotion = None

while True:
    # Read a frame from the webcam
    i, im = webcam.read()
    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

    # Detect faces in the frame
    faces = face_cascade.detectMultiScale(im, 1.3, 5)

    try:
        #iterates over the faces 
        for (x, y, width, height) in faces:
            face_region = gray[y:y + height, x:x + width]
            cv2.rectangle(im, (x, y), (x + width, y + height), (255, 0, 0), 2)
            
            #does the preprocessing necessary to change images to gray and resizes them
            face_region_resized = cv2.resize(face_region, (48, 48))
            feature = np.array(face_region_resized)
            feature = feature.reshape(1, 48, 48, 1)
            features = feature / 255.0
            
            #prediction on the model
            predicted_emotion = predict_emotion(model, features)
            
            #displays the predicted emotion
            display_predicted_emotion(im, predicted_emotion, x, y)

            last_emotion = predicted_emotion

            break

        # Display the output frame
        cv2.imshow("Output", im)
        key = cv2.waitKey(27)
        
        # Exit the loop if the user presses the 'ESC' key
        if key == 27:
            break

    except cv2.error:
        print(f"OpenCV error: {e}")
        break

#Releases the camera and destroys the window for the user
webcam.release()
cv2.destroyAllWindows()

# Print the last recognized emotion when the loop exits
if last_emotion is not None:
    print("Spotify Playlist Being Curated For:", last_emotion)
else:
    print("Unfortunately no faces were seen.")
    exit()

##########

import pandas as pd
import spotipy
from spotipy.oauth2 import SpotifyOAuth

df = pd.read_csv('songmoods.csv')

# The neural network has 7 emotions, but the song dataset has only 4,
# so we need to map the emotions to mood. This will likely result in
# some interesting playlists

match last_emotion:
    case "angry" | "surprise":
        mood = "Energetic"
    case "disgust" | "fear" | "sad":
        mood = "Sad"
    case "neutral":
        mood = "Calm"
    case "happy":
        mood = "Happy"

df = df[df['mood'] == mood]
df = df.sample(n=50)

###
# for spotipy to work, the environment variables SPOTIPY_CLIENT_ID, $SPOTIPY_CLIENT_SECRET,
# and $SPOTIPY_REDIRECT_URI need to be set. I ain't giving mine.

scope = "playlist-modify-private"
sp = spotipy.Spotify(auth_manager=SpotifyOAuth(scope=scope))
user_id = sp.me()['id']

playname = mood + " playlist"
playlist = sp.user_playlist_create(user_id, playname, public=False, collaborative=False, description='Generated by TuneQuest')

tracks=[]
for track in df.id:
    tracks.append("spotify:track:" + track)

sp.playlist_add_items(playlist['id'], tracks)
